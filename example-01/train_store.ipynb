{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Batch Scoring pipeline example\n",
    "\n",
    "In this example, we'll build a `ParallelRunStep` pipeline that can train many small models for time series. The potentials product IDs, SKUs, etc. for which a model should be trained is configured via a configration dataset. This dataset needs to container all the permutations for which a model should be trained. See [`../sample-data/config.csv`](../sample-data/config.csv) for an example, where in total a 1000 models would be trained, one for each code and company permution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azureml-sdk --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Dataset, RunConfiguration\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\n",
    "\n",
    "print(\"Azure ML SDK version:\", azureml.core.VERSION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will connect to the workspace. The command `Workspace.from_config()` will either:\n",
    "* Read the local `config.json` with the workspace reference (given it is there) or\n",
    "* Use the `az` CLI to connect to the workspace and use the workspace attached to via `az ml folder attach -g <resource group> -w <workspace name>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(f'WS name: {ws.name}\\nRegion: {ws.location}\\nSubscription id: {ws.subscription_id}\\nResource group: {ws.resource_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "\n",
    "Let's create the config dataset for making the pipeline work, in this case this is just a CSV that we register as a `TabularDataset`. This is required as input to `ParallelRunStep`, which either parallizes over rows or over a set of files (when using `FileDataset`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload(src_dir='../sample-data', target_path='config', overwrite=True)\n",
    "ds = Dataset.Tabular.from_delimited_files(path=[(datastore, 'config/config.csv')])\n",
    "ds.register(ws, name='config', description='Configuration dataset for ParallelRunStep', create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can prepare our config dataset as input for `ParallelRunStep`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dataset = Dataset.get_by_name(ws, \"config\")\n",
    "config_dataset_consumption = DatasetConsumptionConfig(\"config_dataset\", config_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a output folder that will hold our models. This gives us complete freedom where we want to store the models, so we can easily consume them in a later step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()\n",
    "models = OutputFileDatasetConfig(name='prs_zipped_models',\n",
    "                                 destination=(datastore, 'prs_zipped_models/{run-id}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create a `ParallelRunStep` that runs our trainnig code in parallel on one or more nodes. In this case, we use a `ParallelRunConfig` from a YAML file, that defines our batch scoring job (source script, environement, parallelization, target cluster, etc.). As an output, we'll store all models as zipped batches in a pre-defined folder on Blob or ADLSg2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_run_config = ParallelRunConfig.load_yaml(workspace=ws, path=\"parallel_runconfig.yml\")\n",
    "\n",
    "prs_step = ParallelRunStep(\n",
    "    name=\"parallel-run-step\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    arguments=['--forecast_horizon', '4', '--model_path', models],\n",
    "    inputs=[config_dataset_consumption], # Parallelized minibatch from dataset\n",
    "    side_inputs=[],\n",
    "    output=models, # Will store the models as zip files in this output destination\n",
    "    allow_reuse=False\n",
    ")\n",
    "\n",
    "steps = [prs_step]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create our pipeline object, validate it and then run it against an experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline.validate()\n",
    "\n",
    "pipeline_run = Experiment(ws, 'prs-pipeline').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('azureml': conda)",
   "display_name": "Python 3.7.9 64-bit ('azureml': conda)",
   "metadata": {
    "interpreter": {
     "hash": "54b76a1167e0a2b6a6b8c7f2df323eb2ecfae9d2bbefe58fb0609bf9141d6860"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}